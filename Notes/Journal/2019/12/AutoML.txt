Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2019-12-18T13:20:11+13:00

====== AutoML ======
Created Wednesday 18 December 2019

=== # ML-plan: JAVA ===
----------------

https://github.com/fmohr/ML-Plan 

paper: https://link.springer.com/article/10.1007/s10994-018-5735-z 

abstract: 

Automated machine learning (AutoML) seeks to automatically select, compose, and parametrize machine learning algorithms, so as to achieve optimal performance on a given task (dataset). Although current approaches to AutoML have already produced impressive results, the field is still far from mature, and new techniques are still being developed. In this paper, we present ML-Plan, a new approach to AutoML based on hierarchical planning. To highlight the potential of this approach, we compare ML-Plan to the state-of-the-art frameworks **Auto-WEKA, auto-sklearn, and TPOT**. **In an extensive series of experiments, we show that ML-Plan is highly competitive and often outperforms existing approaches**.

=== # Auto-sklearn: PYTHON ===
----------------------

https://automl.github.io/auto-sklearn/master/ 

paper: https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning

abstract: 

The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. **In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters)**. This system, which we dub auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn.

=== # TPOT: PYTHON ===
-----------------

http://epistasislab.github.io/tpot/

papers:

https://link.springer.com/chapter/10.1007/978-3-319-31204-0_9
https://dl.acm.org/citation.cfm?id=2908918

==== # AUTOGLUON: PYTHON ====
-------------------------

https://autogluon.mxnet.io/index.html#

==== # ML-Plan ====
-------------------------

https://link.springer.com/article/10.1007/s10994-018-5735-z

https://github.com/fmohr/ML-Plan




== ================================================================================================================================================= ==

===== blog posts =====

https://sites.google.com/view/raybellwaves/blog/automl 

and [[~/research/Smart_Ideas/resources/AutoML/Ray Bell - AutoML (2020-02-10 10_21_12 AM).html]]

==== LUDWIG (from UBER) ====

https://uber.github.io/ludwig/








