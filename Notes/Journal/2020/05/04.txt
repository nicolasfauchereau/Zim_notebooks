Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2020-05-04T10:08:40+12:00

====== Monday 04 May 2020 ======

==== Smart Ideas reporting ====

@SI 

C01X1813 report (END19101) living document on 

https://niwa-my.sharepoint.com/:w:/r/personal/ben_noll_niwa_co_nz/_layouts/15/guestaccess.aspx?e=4%3AXx1uVS&at=9&CID=79fe6556-e4a9-ef74-3d41-040ffbfb6621&share=EX0omtUkUPZLj0ct7STlj1wBeUBdbda9bm1aIey6YBxNwA 

==== Smart Ideas: reply to Eibe, fix issues in datasets ====

see email dated 4 May to Eibe, issues related to potential data leakage have been fixed ... 

==== Smart Ideas: re-run of the autoML (AutoGLUON, PYCARET) tasks for classification of NZ 6 regions with updated data ====

see tracking excel file:  [[/home/nicolasf/research/Smart_Ideas/code/models/Model_tracking.xlsx]] 

=== reply from Eibe regarding the PYCARET notebook ===

//Hi Nico,//

//I had a look at your PyCaret notebook, and that library seems quite nice. I haven’t come across it before. (It seems to be brand new?)//

//How do you merge predictions from different GCM-based inputs at test time in that notebook?//

//I don’t think standard k-fold cross-validation is a good way to estimate performance if you have instances for the same time period from different GCMs. Some of those may end up in a training set and the others in a test set. The cross-validation needs to be stratified by time it seems so that all data for a particular time period is either in the training data or the test data?//

//Regarding the results in “Out[25]”, the ranking would change significantly by tuning each of the algorithms. For example, tuning is essential for SVMs and boosting. All the algorithms should outperform naive Bayes.//

**A promising strategy would be to tune a set of algorithms individually and then blend them**//. Here is a reduced set of algorithms that I would consider for this, picking one from each group of highly related classifiers (note that I don’t know what “Ridge Classifier” is. If it refers to “Ridge Logistic Regression”, I would drop “Logistic Regression" from this list as well. I assume that “Extreme Gradient Boosting” refers to XGBoost. That one, appropriately tuned, is probably sufficient to represent the group of boosting models.//

**0 	MLP Classifier 				0.6141 	0.0 	0.6031 	0.6086 	0.6027 	0.4167**
**1 	Extra Trees Classifier 			0.5940 	0.0 	0.5801 	0.5641 	0.5697 	0.3823**
**3 	Quadratic Discriminant Analysis 	0.5501 	0.0 	0.5423 	0.5474 	0.5405 	0.3229**
**4 	K Neighbors Classifier 			0.5353 	0.0 	0.5201 	0.5171 	0.5140 	0.2921**
----------------------------------------------------------------------------------------------**6 	Linear Discriminant Analysis 		0.5086 	0.0 	0.4977 	0.4895 	0.4934 	0.2579**
**7 	Ridge Classifier 			0.5085 	0.0 	0.4967 	0.4912 	0.4896 	0.2565**
----------------------------------------------------------------------------------------------**9 	Logistic Regression 			0.5018 	0.0 	0.4893 	0.4859 	0.4860 	0.2474**
----------------------------------------------------------------------------------------------**11 	Extreme Gradient Boosting 		0.4913 	0.0 	0.4821 	0.4804 	0.4803 	0.2314**
**12 	Naive Bayes 				0.4644 	0.0 	0.4565 	0.4693 	0.4574 	0.1930**
----------------------------------------------------------------------------------------------**16 	SVM - Radial Kernel 			0.3755 	0.0 	0.3333 	0.1410 	0.2051 	0.0000**
						
//Cheers,//
//Eibe//

**→ see potential for shuffling all the training and test instances, using the sample method of a dataframe, eg.** 

if shuffle: 
	train_data = train_data.sample(frac=1.)
	test_date = train_data.sample(frac=1.)





